{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFtKVjj+/DXR7Fc1LYZH3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhnidh/Assessment-of-Fruit-Quality-using-Hyperspectral-Imaging-with-the-help-of-Deep-learning/blob/main/Alexnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEICd3XbgTFB"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "# import metric\n",
        "from keras.metrics import sparse_categorical_crossentropy\n",
        "# metric\n",
        "from keras.metrics import binary_crossentropy\n",
        "# optimization method (Stochastic Gradient Descent (SGD))\n",
        "from keras.optimizers import SGD\n",
        "# optimization method\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pathlib\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "from zipfile import ZipFile\n",
        "import cv2\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.image as mpimg\n",
        "from numpy import *\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATADIR = 'D:/archivefruit/fruits-360_dataset/fruits-360/Training'\n",
        "DATADIR ='E:\\project\\set project\\SET 1\\Fruit-classification-and-analysis-using-hyperspectral-images-and-deep-learning-main\\data'\n",
        "CATEGORIES = [\"Apple Braeburn\",\"Avocado\",\"Banana\",\"Blueberry\",\"Guava\",\"Kiwi\",\"Orange\"]\n",
        "for category in CATEGORIES:\n",
        "    path = os.path.join(DATADIR, category)\n",
        "    for img in os.listdir(path):\n",
        "        img_array = cv2.imread(os.path.join(path,img))\n",
        "        #,cv2.IMREAD_GRAYSCALE\n",
        "        plt.imshow(img_array)\n",
        "        plt.show()\n",
        "        break\n",
        "   # break\n"
      ],
      "metadata": {
        "id": "QZpJrQYghEz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building our training data\n",
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  # do fruits\n",
        "\n",
        "        path = os.path.join(DATADIR,category)  # create path to fruits\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 ,1,2,3,4,5,6) different number donate different category of fruit\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per fruits\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                pass\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "image_count = len(training_data)"
      ],
      "metadata": {
        "id": "hVvYSt6IhVJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_count)"
      ],
      "metadata": {
        "id": "BCsegTK6hY7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle the dataset\n",
        "random.shuffle(training_data)"
      ],
      "metadata": {
        "id": "lpMmo16AhZjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in training_data[:10]:\n",
        "    print(sample[1])"
      ],
      "metadata": {
        "id": "V6RGOr3GhevY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 100\n",
        "batch_size = 128\n",
        "img_height = 100\n",
        "img_width = 100"
      ],
      "metadata": {
        "id": "cmVNaAOKhfXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning labels and features\n",
        "X =[]\n",
        "y =[]\n",
        "for features, label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)"
      ],
      "metadata": {
        "id": "IhY6hxD4hfZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating own model\n",
        "#Normalising X and converting labels to categorical data\n",
        "X = X.astype('float32')\n",
        "X /= 255\n",
        "Y = np_utils.to_categorical(y,7)\n",
        "print(Y[100])\n",
        "print(shape(Y))"
      ],
      "metadata": {
        "id": "vXaAMPVDhfeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X)\n",
        "print(type(X))"
      ],
      "metadata": {
        "id": "iIj8bfWghfhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.array(y)\n",
        "print(type(y))"
      ],
      "metadata": {
        "id": "P7AfXq_hhvhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 7)"
      ],
      "metadata": {
        "id": "RlAYHTk1hyxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define compare and train the CNN model\n",
        "batch_size = 128\n",
        "nb_classes =7\n",
        "nb_epochs = 10\n",
        "img_rows, img_columns = 100, 100\n",
        "img_channel = 3\n",
        "nb_filters = 32\n",
        "nb_pool = 2\n",
        "nb_conv = 3"
      ],
      "metadata": {
        "id": "eYu0Po1wh0Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Alexnet():\n",
        "\n",
        "    # Initialize the model\n",
        "    model = Sequential()\n",
        "\n",
        "    # layer 1: convolutional layer + max-pooling layer\n",
        "    model.add(Conv2D(filters = 96, kernel_size = (11,11), strides= 4, padding = 'valid', activation='relu', input_shape = (100,100,1)))\n",
        "    model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "    # layer 2: convolutional layer + max-pooling layer\n",
        "    model.add(Conv2D(filters = 256, kernel_size = (5,5), padding = 'same', activation = 'relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "    # layers 3-5: three convolutional layers + 1 max-pooling layer\n",
        "    model.add(Conv2D(filters = 384, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "    model.add(Conv2D(filters = 384, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "    model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (3,3), strides = 2))\n",
        "\n",
        "    # layers 6 - 8: two fully connected hidden layers and one fully connected output layer\n",
        "    model.add(Dense(4096, activation = 'relu',input_dim=4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation = 'relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1000, activation = 'softmax'))\n",
        "\n",
        "    # compile the model with a loss funciton, a metric and optimizer method for estimating the loss function\n",
        "    opt = SGD(lr = 0.1)\n",
        "    model.compile(loss = sparse_categorical_crossentropy,optimizer = opt,metrics = ['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rwB6yPoXh8VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Alexnet_model = Alexnet()\n",
        "Alexnet_model.summary()"
      ],
      "metadata": {
        "id": "67DVWXgah9CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test, epochs, batch_size):\n",
        "  # Data generator\n",
        "  datagen = ImageDataGenerator(rotation_range = 5, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True)\n",
        "  # iteration on the training set\n",
        "  it_train = datagen.flow(X_train, y_train, batch_size = batch_size)\n",
        "  # path to save the checkpoint\n",
        "  path_cp = os.getcwd() + '/' + 'weights_.hdf5'\n",
        "  checkpoint_ = ModelCheckpoint(path_cp, monitor = 'loss', save_best_only = True, mode = 'auto')\n",
        "  steps = X_train.shape[0]//batch_size\n",
        "  # Fitting the model\n",
        "  history = model.fit(it_train, epochs = epochs, steps_per_epoch = steps, validation_data = (X_test, y_test), verbose = 1, callbacks = checkpoint_)\n",
        "  # Evaluating the model\n",
        "  _, acc = model.evaluate(X_test, y_test, verbose = 1)\n",
        "  print('%.3f' % (acc * 100.0))\n",
        "\n",
        "  return history, acc\n",
        "\n",
        "train_history, acc = train_model(Alexnet_model, X_train, y_train, X_test, y_test, batch_size = 32, epochs = 10)"
      ],
      "metadata": {
        "id": "at0u6Dg-iBKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# convert the history.history to a pandas DataFrame:\n",
        "train_hist_df = pd.DataFrame(train_history.history)\n",
        "\n",
        "train_dict = {'train_hist': train_hist_df, 'acc': acc}\n",
        "np.save('train_dict.npy', train_dict)"
      ],
      "metadata": {
        "id": "fQk817iZiMSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Alexnet_model.save('Alexnet_model.h5')"
      ],
      "metadata": {
        "id": "KybziO_yiMXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy(history):\n",
        "  plt.figure(figsize = (5,5))\n",
        "  plt.plot(history .history['accuracy'], color = 'blue', label = 'train')\n",
        "  plt.plot(history.history['val_accuracy'], color = 'orange', label = 'val')\n",
        "  plt.legend()\n",
        "  plt.title('Accuracy')\n",
        "  plt.show()\n",
        "\n",
        "plot_accuracy(train_history)"
      ],
      "metadata": {
        "id": "WsSYg6_ciMeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy and Score of model\n",
        "score = Alexnet_model.evaluate(X_test, y_test, verbose = 1 )\n",
        "print(\"Test Score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1]*100,'%')"
      ],
      "metadata": {
        "id": "2CQjpkweiXHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#observing its classification report and confusion matrix\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "#predict\n",
        "y_pred=Alexnet_model.predict(X_test)\n",
        "y_pred=np.argmax(y_pred,axis=1)\n",
        "print(classification_report(y_pred,y_test))"
      ],
      "metadata": {
        "id": "VAHgyIHCiXNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_pred,y_test))"
      ],
      "metadata": {
        "id": "qDXf3Se9iXQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get classification report\n",
        "print(classification_report(y_pred,y_test))"
      ],
      "metadata": {
        "id": "8ZmJ4l9Yiibg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get confusion matrix\n",
        "print(confusion_matrix(y_pred,y_test))"
      ],
      "metadata": {
        "id": "fTQatmbuiigq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Reverse the true and the predicted labels into the original binary\n",
        "# labels (0 and 1) to compute the confusion matrix\n",
        "y_test_labels = [np.argmax(vect) for vect in y_test]\n",
        "y_test_pred_labels = [np.argmax(vect) for vect in y_test_labels]\n",
        "\n",
        "conf_mat = confusion_matrix(y_test_labels, y_test_pred_labels)\n",
        "\n",
        "plt.figure(figsize = (10,8))\n",
        "sns.heatmap(conf_mat, linewidths = 0.1, cmap = 'Greens', linecolor = 'gray',\n",
        "            fmt = '.1f', annot = True)\n",
        "plt.xlabel('Predicted classes', fontsize = 20)\n",
        "plt.ylabel('True classes', fontsize = 20)"
      ],
      "metadata": {
        "id": "HOakgeeYiii1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,7))\n",
        "for i in np.arange(7):\n",
        "  # take randomly an indice\n",
        "  ind = random.randint(0, len(X_test))\n",
        "  img = X_test[ind]\n",
        "  # reshape the image\n",
        "  img_rs = img.reshape(1,100,100,1)\n",
        "  # predict the label of img\n",
        "  y_pred = Alexnet_model.predict(img_rs)\n",
        "  # determine the corresponding category\n",
        "  predicted_cate = category[np.argmax(y_pred)]\n",
        "  plt.subplot(240+1+i)\n",
        "  plt.imshow(img)\n",
        "  plt.title('predicted: ' + str(predicted_cate))"
      ],
      "metadata": {
        "id": "Ix0SHDDAisKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ebWugkSzisNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ST3i9XisisRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHwwpJ8_isTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ULxxGcpisXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_Sd2tDBiim0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}